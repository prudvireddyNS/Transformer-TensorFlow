{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4ec8d5-7726-45ec-9a5f-cfd5c505798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "492e26f5-b637-48fa-90ef-c4fe91b35c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask = None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = tf.matmul(q, tf.transpose(k, perm = [0,1,3,2])) / np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = tf.keras.activations.softmax(scaled, axis=-1)\n",
    "    values = tf.matmul(attention, v)\n",
    "    return attention, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "82c044f8-3a98-4cc0-9c14-415429cb6c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model)\n",
    "        self.linear_layer = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x, mask = None):\n",
    "        batch_size, sequence_length, d_model = x.shape\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(qkv.shape)\n",
    "        print(self.num_heads, self.head_dim)\n",
    "        qkv = tf.reshape(qkv, [batch_size, sequence_length, self.num_heads, self.head_dim * 3])\n",
    "        qkv = tf.transpose(qkv, perm=[0,2,1,3])\n",
    "        q, k, v = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
    "        attention, values = scaled_dot_product(q, k, v)\n",
    "        values = tf.reshape(values, [batch_size, sequence_length, self.num_heads * self.head_dim])\n",
    "        out = self.linear_layer(values)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b6bb1443-0a23-476b-ab02-6329c8bd64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, parameters_shape, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = tf.Variable(tf.ones(parameters_shape))\n",
    "        self.beta = tf.Variable(tf.zeros(parameters_shape))\n",
    "\n",
    "    def call(self, x):\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
    "        mean = tf.reduce_mean(x, axis=dims, keepdims=True)\n",
    "        var = tf.reduce_mean((x - mean)**2, axis = dims, keepdims=True)\n",
    "        std = tf.sqrt(var + self.eps)\n",
    "        y = (x - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "64057486-8dd2-4ec9-9032-ba86815d052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, hidden, drop_prob):\n",
    "        super().__init__()\n",
    "        self.linear1 = tf.keras.layers.Dense(hidden, activation='relu')\n",
    "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fd595a5d-9baa-4ae9-8fdc-50761751ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(d_model = d_model, num_heads = num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape = [d_model])\n",
    "        self.dropout1 = tf.keras.layers.Dropout(drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model = d_model, hidden = ffn_hidden, drop_prob = drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape = [d_model])\n",
    "        self.dropout2 = tf.keras.layers.Dropout(drop_prob)\n",
    "\n",
    "    def call(self, x):\n",
    "        residual_x = x\n",
    "        print(\"--------------ATTENTION 1----------------\")\n",
    "        x = self.attention(x)\n",
    "        print(\"--------------DROPOUT 1------------------\")\n",
    "        x = self.dropout1(x)\n",
    "        print(\"--------------ADD and LAYER NORM---------\")\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x\n",
    "        print(\"--------------ATTENTION 2----------------\")\n",
    "        x = self.ffn(x)\n",
    "        print(\"--------------DROPOUT 2------------------\")\n",
    "        x = self.dropout2(x)\n",
    "        print(\"--------------ADD and LAYER NORM---------\")\n",
    "        x = self.norm2(x + residual_x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0544888b-5fdb-4a0b-8150-07f06a162f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder_layers = tf.keras.Sequential([EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def call(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            encoder_layer = self.encoder_layers.layers[i]\n",
    "            print(f\"\\n--------------Layer {i+1}----------------\\n\")\n",
    "            x = encoder_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a1dab9f1-3c75-4a02-9fa1-2f8c22fd35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 200\n",
    "batch_size = 30\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "ffn_hidden = 2048\n",
    "num_layers = 5\n",
    "\n",
    "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7621227e-fb23-4f34-9a84-4b2b5e5b7eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------Layer 1----------------\n",
      "\n",
      "--------------ATTENTION 1----------------\n",
      "(30, 200, 1536)\n",
      "8 64\n",
      "--------------DROPOUT 1------------------\n",
      "--------------ADD and LAYER NORM---------\n",
      "--------------ATTENTION 2----------------\n",
      "--------------DROPOUT 2------------------\n",
      "--------------ADD and LAYER NORM---------\n",
      "\n",
      "--------------Layer 2----------------\n",
      "\n",
      "--------------ATTENTION 1----------------\n",
      "(30, 200, 1536)\n",
      "8 64\n",
      "--------------DROPOUT 1------------------\n",
      "--------------ADD and LAYER NORM---------\n",
      "--------------ATTENTION 2----------------\n",
      "--------------DROPOUT 2------------------\n",
      "--------------ADD and LAYER NORM---------\n",
      "\n",
      "--------------Layer 3----------------\n",
      "\n",
      "--------------ATTENTION 1----------------\n",
      "(30, 200, 1536)\n",
      "8 64\n",
      "--------------DROPOUT 1------------------\n",
      "--------------ADD and LAYER NORM---------\n",
      "--------------ATTENTION 2----------------\n",
      "--------------DROPOUT 2------------------\n",
      "--------------ADD and LAYER NORM---------\n",
      "\n",
      "--------------Layer 4----------------\n",
      "\n",
      "--------------ATTENTION 1----------------\n",
      "(30, 200, 1536)\n",
      "8 64\n",
      "--------------DROPOUT 1------------------\n",
      "--------------ADD and LAYER NORM---------\n",
      "--------------ATTENTION 2----------------\n",
      "--------------DROPOUT 2------------------\n",
      "--------------ADD and LAYER NORM---------\n",
      "\n",
      "--------------Layer 5----------------\n",
      "\n",
      "--------------ATTENTION 1----------------\n",
      "(30, 200, 1536)\n",
      "8 64\n",
      "--------------DROPOUT 1------------------\n",
      "--------------ADD and LAYER NORM---------\n",
      "--------------ATTENTION 2----------------\n",
      "--------------DROPOUT 2------------------\n",
      "--------------ADD and LAYER NORM---------\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((batch_size, sequence_length,d_model))\n",
    "out = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb5ebfc-6114-4891-bbac-1e9a885719cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
